# Tutoriel d'Apprentissage par Renforcement (DQN)

## Guide Étape par Étape en Français

### Introduction

Ce guide vous accompagne dans l'implémentation d'un agent Deep Q-Learning (DQN) pour résoudre le problème CartPole-v1 avec PyTorch. L'agent doit maintenir un poteau en équilibre sur un chariot mobile en choisissant entre deux actions : déplacer le chariot à gauche ou à droite.

---

## Étape 1 : Installation et Importation des Bibliothèques

### 1.1 Installation de Gymnasium

Ouvrez votre terminal et exécutez :

```bash
pip install gymnasium[classic_control]
pip install torch matplotlib
```

### 1.2 Importation des Modules

```python
import gymnasium as gym
import math
import random
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple, deque
from itertools import count

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
```

**Explication :** Ces bibliothèques fournissent l'environnement de simulation (gymnasium), les outils de réseau neuronal (torch), et la visualisation (matplotlib).

---

## Étape 2 : Configuration de l'Environnement

### 2.1 Création de l'Environnement

```python
env = gym.make("CartPole-v1")
```

**Explication :** Crée l'environnement CartPole où notre agent va apprendre.

### 2.2 Configuration de Matplotlib

```python
is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython:
    from IPython import display

plt.ion()  # Active le mode interactif
```

### 2.3 Sélection du Dispositif (GPU/CPU)

```python
device = torch.device(
    "cuda" if torch.cuda.is_available() else
    "mps" if torch.backends.mps.is_available() else
    "cpu"
)
```

**Explication :** Utilise le GPU si disponible pour accélérer l'entraînement, sinon utilise le CPU.

---

## Étape 3 : Mémoire de Rejeu (Replay Memory)

### 3.1 Définition de la Structure Transition

```python
Transition = namedtuple('Transition',
                        ('state', 'action', 'next_state', 'reward'))
```

**Explication :** Un tuple nommé qui stocke une transition : état actuel, action prise, état suivant, et récompense obtenue.

### 3.2 Classe ReplayMemory

```python
class ReplayMemory(object):
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        """Sauvegarde une transition"""
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)
```

**Explication :** Cette classe stocke les expériences passées de l'agent. En échantillonnant aléatoirement, nous décorrélons les transitions, ce qui stabilise l'entraînement.

---

## Étape 4 : Construction du Réseau Neuronal Q

### 4.1 Classe DQN

```python
class DQN(nn.Module):
    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(n_observations, 128)
        self.layer2 = nn.Linear(128, 128)
        self.layer3 = nn.Linear(128, n_actions)

    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        return self.layer3(x)
```

**Explication :** Ce réseau neuronal prend l'état de l'environnement en entrée et produit les valeurs Q pour chaque action possible. Il comprend trois couches entièrement connectées avec des fonctions d'activation ReLU.

---

## Étape 5 : Définition des Hyperparamètres

```python
BATCH_SIZE = 128        # Nombre de transitions par batch
GAMMA = 0.99            # Facteur d'actualisation
EPS_START = 0.9         # Valeur initiale d'epsilon (exploration)
EPS_END = 0.01          # Valeur finale d'epsilon
EPS_DECAY = 2500        # Vitesse de décroissance d'epsilon
TAU = 0.005             # Taux de mise à jour du réseau cible
LR = 3e-4               # Taux d'apprentissage
```

**Explication :** Ces hyperparamètres contrôlent le comportement de l'algorithme d'apprentissage.

---

## Étape 6 : Initialisation des Réseaux

### 6.1 Obtention des Dimensions

```python
n_actions = env.action_space.n
state, info = env.reset()
n_observations = len(state)
```

### 6.2 Création des Réseaux Policy et Target

```python
policy_net = DQN(n_observations, n_actions).to(device)
target_net = DQN(n_observations, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict())
```

**Explication :** Le `policy_net` est utilisé pour choisir les actions, tandis que le `target_net` fournit des valeurs cibles stables pour l'entraînement.

### 6.3 Configuration de l'Optimiseur et de la Mémoire

```python
optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
memory = ReplayMemory(10000)
steps_done = 0
```

---

## Étape 7 : Sélection d'Action (Stratégie Epsilon-Greedy)

```python
def select_action(state):
    global steps_done
    sample = random.random()
    eps_threshold = EPS_END + (EPS_START - EPS_END) * \
        math.exp(-1. * steps_done / EPS_DECAY)
    steps_done += 1
    
    if sample > eps_threshold:
        with torch.no_grad():
            return policy_net(state).max(1).indices.view(1, 1)
    else:
        return torch.tensor([[env.action_space.sample()]], 
                          device=device, dtype=torch.long)
```

**Explication :** Cette fonction choisit une action soit aléatoirement (exploration) soit en utilisant le réseau neuronal (exploitation). La probabilité d'exploration diminue au fil du temps selon la décroissance exponentielle d'epsilon.

---

## Étape 8 : Visualisation des Performances

```python
episode_durations = []

def plot_durations(show_result=False):
    plt.figure(1)
    durations_t = torch.tensor(episode_durations, dtype=torch.float)
    
    if show_result:
        plt.title('Résultat')
    else:
        plt.clf()
        plt.title('Entraînement en cours...')
    
    plt.xlabel('Épisode')
    plt.ylabel('Durée')
    plt.plot(durations_t.numpy())
    
    if len(durations_t) >= 100:
        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)
        means = torch.cat((torch.zeros(99), means))
        plt.plot(means.numpy())

    plt.pause(0.001)
    if is_ipython:
        if not show_result:
            display.display(plt.gcf())
            display.clear_output(wait=True)
        else:
            display.display(plt.gcf())
```

**Explication :** Cette fonction trace la durée de chaque épisode et calcule une moyenne mobile sur 100 épisodes pour suivre les progrès.

---

## Étape 9 : Fonction d'Optimisation

```python
def optimize_model():
    if len(memory) < BATCH_SIZE:
        return
    
    transitions = memory.sample(BATCH_SIZE)
    batch = Transition(*zip(*transitions))

    # Masque pour les états non-terminaux
    non_final_mask = torch.tensor(
        tuple(map(lambda s: s is not None, batch.next_state)), 
        device=device, dtype=torch.bool
    )
    non_final_next_states = torch.cat([s for s in batch.next_state
                                        if s is not None])
    
    state_batch = torch.cat(batch.state)
    action_batch = torch.cat(batch.action)
    reward_batch = torch.cat(batch.reward)

    # Calcul de Q(s_t, a)
    state_action_values = policy_net(state_batch).gather(1, action_batch)

    # Calcul de V(s_{t+1})
    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    with torch.no_grad():
        next_state_values[non_final_mask] = target_net(
            non_final_next_states
        ).max(1).values
    
    # Calcul des valeurs Q attendues
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch

    # Calcul de la perte de Huber
    criterion = nn.SmoothL1Loss()
    loss = criterion(state_action_values, 
                    expected_state_action_values.unsqueeze(1))

    # Optimisation
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)
    optimizer.step()
```

**Explication :** Cette fonction effectue une étape d'optimisation en échantillonnant un batch de la mémoire, en calculant la perte (erreur de différence temporelle) et en mettant à jour les poids du réseau.

---

## Étape 10 : Boucle d'Entraînement Principale

```python
if torch.cuda.is_available() or torch.backends.mps.is_available():
    num_episodes = 600
else:
    num_episodes = 50

for i_episode in range(num_episodes):
    # Initialisation de l'environnement
    state, info = env.reset()
    state = torch.tensor(state, dtype=torch.float32, 
                        device=device).unsqueeze(0)
    
    for t in count():
        # Sélection et exécution d'une action
        action = select_action(state)
        observation, reward, terminated, truncated, _ = env.step(action.item())
        reward = torch.tensor([reward], device=device)
        done = terminated or truncated

        if terminated:
            next_state = None
        else:
            next_state = torch.tensor(observation, dtype=torch.float32, 
                                     device=device).unsqueeze(0)

        # Stockage de la transition
        memory.push(state, action, next_state, reward)

        # Passage à l'état suivant
        state = next_state

        # Optimisation du modèle
        optimize_model()

        # Mise à jour douce du réseau cible
        target_net_state_dict = target_net.state_dict()
        policy_net_state_dict = policy_net.state_dict()
        for key in policy_net_state_dict:
            target_net_state_dict[key] = (policy_net_state_dict[key] * TAU + 
                                         target_net_state_dict[key] * (1 - TAU))
        target_net.load_state_dict(target_net_state_dict)

        if done:
            episode_durations.append(t + 1)
            plot_durations()
            break

print('Entraînement terminé')
plot_durations(show_result=True)
plt.ioff()
plt.show()
```

**Explication :** Cette boucle entraîne l'agent sur plusieurs épisodes. À chaque étape, l'agent choisit une action, observe le résultat, stocke l'expérience, optimise le réseau, et met à jour le réseau cible.

---

## Comprendre l'Algorithme DQN

### Principes Fondamentaux

**1. Fonction Q**
La fonction Q estime le retour total attendu en prenant une action `a` dans un état `s` :
- Q(s, a) = récompense future attendue

**2. Équation de Bellman**
La fonction Q satisfait l'équation :
- Q(s, a) = r + γ × max Q(s', a')

où :
- r = récompense immédiate
- γ = facteur d'actualisation (GAMMA)
- s' = état suivant
- a' = action suivante

**3. Erreur de Différence Temporelle**
- δ = Q(s, a) - (r + γ × max Q(s', a'))

L'objectif est de minimiser cette erreur.

### Composants Clés

**Mémoire de Rejeu**
Stocke les expériences passées pour un apprentissage plus stable en décorrélant les données d'entraînement.

**Réseau Cible**
Un réseau séparé mis à jour lentement pour fournir des valeurs cibles stables lors de l'entraînement.

**Exploration vs Exploitation**
La stratégie epsilon-greedy équilibre l'exploration (actions aléatoires) et l'exploitation (actions basées sur Q).

---

## Conseils pour l'Exécution

1. **Entraînement Initial** : Les premières centaines d'épisodes peuvent sembler instables. C'est normal.

2. **Convergence** : L'agent devrait atteindre des durées d'épisode de 500 pas après environ 400-600 épisodes.

3. **Réglage** : Si la convergence n'est pas observée, essayez de :
   - Augmenter le nombre d'épisodes
   - Ajuster le taux d'apprentissage (LR)
   - Modifier EPS_DECAY pour changer la vitesse d'exploration

4. **Reproductibilité** : Pour des résultats reproductibles, décommentez le code de seed dans le fichier original.

---

## Résultats Attendus

Après un entraînement réussi, vous devriez observer :
- Augmentation progressive de la durée des épisodes
- Stabilisation autour de 400-500 pas par épisode
- Convergence de la moyenne mobile sur 100 épisodes

Le graphique final montrera clairement l'amélioration des performances de l'agent au fil du temps.

---

## Conclusion

Vous avez maintenant implémenté un agent DQN complet capable d'apprendre à résoudre le problème CartPole. Cette approche peut être adaptée à d'autres environnements et constitue une base solide pour l'apprentissage par renforcement profond.

**Prochaines Étapes** :
- Expérimentez avec différents hyperparamètres
- Essayez d'autres environnements Gymnasium
- Implémentez des variantes comme Double DQN ou Dueling DQN
